from rest_framework import generics, status, filters, pagination
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.response import Response
from .models import Knowledge
from .serializers import KnowledgeSerializer,EnterpriseQuerySerializer,EnterpriseQueryResponseSerializer
import pdfplumber
import pytesseract
import docx
import PyPDF2
from pdf2image import convert_from_path
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from django.conf import settings
from django.core.files.storage import default_storage
from rest_framework.views import APIView
from langchain_core.prompts import PromptTemplate
from .azure_llama_api import AzureLlamaAPI

def extract_text_from_file(file_path):
    """ æ ¹æ“šæ–‡ä»¶é¡å‹è§£æå…§å®¹ """
    text = ""

    if file_path.endswith(".pdf"):
        # **æ–¹æ³• 1ï¼šä½¿ç”¨ pdfplumber è§£æ**
        with pdfplumber.open(file_path) as pdf:
            text = "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])

        # **æ–¹æ³• 2ï¼šä½¿ç”¨ PyPDF2 è§£æ**
        if not text.strip():
            with open(file_path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"

        # **æ–¹æ³• 3ï¼šä½¿ç”¨ OCR è§£æåœ–ç‰‡ PDF**
        if not text.strip():
            images = convert_from_path(file_path)
            for img in images:
                text += pytesseract.image_to_string(img, lang="chi_tra") + "\n"

    elif file_path.endswith(".docx"):
        doc = docx.Document(file_path)
        for para in doc.paragraphs:
            text += para.text + "\n"

    print("è§£æå…§å®¹å‰ 500 å­—:", text[:500])  # æ¸¬è©¦æ˜¯å¦æœ‰è®€å–å…§å®¹
    return text.strip()

def extract_text_from_pdf_with_pages(file_path):
    """è§£æ PDFï¼Œä¸¦è¿”å› (é ç¢¼, æ–‡å­—å…§å®¹) åˆ—è¡¨"""
    extracted_pages = []
    
    with pdfplumber.open(file_path) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            text = page.extract_text()
            if text:
                extracted_pages.append((i, text))

    if not extracted_pages:
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for i, page in enumerate(reader.pages, start=1):
                page_text = page.extract_text()
                if page_text:
                    extracted_pages.append((i, page_text))

    print(f"âœ… è§£æ {len(extracted_pages)} é  PDF å…§å®¹")
    return extracted_pages

# è¨­å®šè©åµŒå…¥æ¨¡å‹
embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# è¨­å®šä¼æ¥­å…§éƒ¨çŸ¥è­˜åº«çš„å„²å­˜è·¯å¾‘
CHROMA_ENTERPRISE_DB_PATH = "chroma_enterprise_db"

# åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
enterprise_vectorstore = Chroma(persist_directory=CHROMA_ENTERPRISE_DB_PATH, embedding_function=embedder)

# æ–‡å­—åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)

def add_to_enterprise_vectorstore(title, content, page_number=1):
    """å°‡æ–‡ä»¶å…§å®¹åˆ‡å‰²å¾Œå­˜å…¥å‘é‡è³‡æ–™åº«ï¼Œä¸¦è¨˜éŒ„é ç¢¼"""
    global enterprise_vectorstore

    if not content.strip():
        print(f"âš ï¸ è·³éå­˜å…¥ä¼æ¥­çŸ¥è­˜åº«ï¼ˆå…§å®¹ç‚ºç©ºï¼‰ï¼š{title}")
        return False

    # æ‹†åˆ†æ–‡æœ¬
    chunks = text_splitter.split_text(content)
    if not chunks:
        print(f"âš ï¸ ç„¡æ³•æ‹†åˆ†æ–‡æœ¬ï¼Œè·³éå­˜å…¥ ChromaDBï¼ˆ{title}ï¼‰")
        return False

    # è¨­å®š metadataï¼ŒåŒ…å«æ–‡ä»¶åç¨±èˆ‡é ç¢¼
    metadata = [{"title": title, "chunk_index": i, "page_number": page_number} for i in range(len(chunks))]

    # å­˜å…¥å‘é‡è³‡æ–™åº«
    enterprise_vectorstore.add_texts(chunks, metadatas=metadata)
    print(f"âœ… ä¼æ¥­çŸ¥è­˜åº«å·²æ–°å¢: {title}ï¼Œå…± {len(chunks)} æ®µè½ (Page {page_number})")
    return True

def delete_from_enterprise_vectorstore(title):
    """
    å¾ ChromaDB åˆªé™¤å°æ‡‰ `title` çš„å‘é‡è³‡æ–™
    """
    try:
        all_vectors = enterprise_vectorstore._collection.get(include=["metadatas"])

        if all_vectors and "metadatas" in all_vectors:
            matching_vectors = [meta for meta in all_vectors["metadatas"] if meta.get("title") == title]

            if matching_vectors:
                print(f"ğŸ” æ‰¾åˆ° `{title}` çš„ {len(matching_vectors)} ç­†å‘é‡è³‡æ–™ï¼Œæº–å‚™åˆªé™¤...")
                enterprise_vectorstore._collection.delete(where={"title": title})
                print(f"âœ… å·²æˆåŠŸåˆªé™¤ `{title}` çš„å‘é‡è³‡æ–™")
                return True
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ° `{title}` çš„å‘é‡è³‡æ–™ï¼Œç„¡éœ€åˆªé™¤")
                return False
    except Exception as e:
        print(f"âŒ åˆªé™¤ `{title}` çš„å‘é‡è³‡æ–™å¤±æ•—: {e}")
        return False

'''
def rebuild_vectorstore():
    """æ¸…ç©ºä¸¦é‡æ–°å»ºç½®ä¼æ¥­çŸ¥è­˜åº«çš„ ChromaDB"""
    global enterprise_vectorstore
    # å–å¾—æ‰€æœ‰å‘é‡çš„ metadata
    all_vectors = enterprise_vectorstore._collection.get(include=["metadatas"])

    if all_vectors and "metadatas" in all_vectors:
        all_titles = {meta["title"] for meta in all_vectors["metadatas"] if "title" in meta}
        # æ‰¹é‡åˆªé™¤
        for title in all_titles:
            enterprise_vectorstore._collection.delete(where={"title": title})
            print(f"âœ… å·²åˆªé™¤å‘é‡è³‡æ–™: {title}")
        print("âš ï¸ å·²æˆåŠŸæ¸…ç©ºä½¿ç”¨è€…å‘é‡è³‡æ–™åº«ï¼")

    # **é‡æ–°è¼‰å…¥æ‰€æœ‰ Knowledge è³‡æ–™**
    all_documents = Knowledge.objects.all()
    if not all_documents.exists():
        print("âš ï¸ æ²’æœ‰ä¼æ¥­æ–‡ä»¶ï¼Œç„¡éœ€é‡å»ºå‘é‡è³‡æ–™åº«ã€‚")
        return

    for doc in all_documents:
        title = os.path.basename(doc.file.name)  # ç¢ºä¿èˆ‡åˆªé™¤æ™‚çš„åç¨±ä¸€è‡´
        add_to_enterprise_vectorstore(title, doc.content)

    print(f"âœ… å·²æˆåŠŸé‡æ–°è¼‰å…¥ {len(all_documents)} ä»½ä¼æ¥­çŸ¥è­˜åº«æ–‡ä»¶ï¼")
'''

class KnowledgePagination(pagination.PageNumberPagination):
    """RESTful API æ¨™æº–åˆ†é """
    page_size = 10  # é è¨­æ¯é  10 ç­†
    page_size_query_param = 'size'  # å…è¨±ä½¿ç”¨è€…èª¿æ•´æ¯é å¤§å°
    max_page_size = 50  # æœ€å¤§ 50 ç­†

class KnowledgeListCreateView(generics.ListCreateAPIView):
    """
    çŸ¥è­˜åº«ç®¡ç†ï¼š
    - `GET` æŸ¥è©¢æ‰€æœ‰çŸ¥è­˜
    - `POST` ä¸Šå‚³æ–°çŸ¥è­˜
    """
    queryset = Knowledge.objects.all().order_by("-created_at")
    serializer_class = KnowledgeSerializer
    pagination_class = None
    parser_classes = (MultiPartParser, FormParser)
    filter_backends = [filters.SearchFilter]
    search_fields = ["file", "department"]

    def get(self, request, *args, **kwargs):
        """æŸ¥è©¢æ‰€æœ‰çŸ¥è­˜ï¼ˆæ”¯æ´ title & department éæ¿¾ï¼‰"""
        queryset = self.filter_queryset(self.get_queryset())

        title = request.GET.get("title")
        department = request.GET.get("department")

        if title:
            queryset = queryset.filter(file__icontains=title)
        if department:
            queryset = queryset.filter(department__icontains=department)

        # **å°‡æŸ¥è©¢çµæœè½‰æ›ç‚º JSON**
        serializer = self.get_serializer(queryset, many=True)

        return Response({"data": serializer.data, "count": len(serializer.data)}, status=status.HTTP_200_OK)

    def post(self, request, *args, **kwargs):
        """ä¸Šå‚³æ–°çŸ¥è­˜æ–‡ä»¶ï¼ˆæ”¯æ´ PDF é ç¢¼æ¨™è¨˜ï¼‰"""
        if "file" not in request.FILES:
            return Response({"error": "è«‹ä¸Šå‚³æ–‡ä»¶"}, status=status.HTTP_400_BAD_REQUEST)

        file = request.FILES["file"]
        department = request.data.get("department", "")
        author = request.data.get("author", None)

        # **å„²å­˜æª”æ¡ˆ**
        knowledge = Knowledge.objects.create(file=file, department=department, author_id=author)
        file_path = knowledge.file.path  # Django è‡ªå‹•å„²å­˜è·¯å¾‘

        # **è§£ææ–‡ä»¶å…§å®¹ï¼ˆè§£æ PDF é ç¢¼ï¼‰**
        if file.name.endswith(".pdf"):
            extracted_pages = extract_text_from_pdf_with_pages(file_path)
        else:
            extracted_pages = [(1, extract_text_from_file(file_path))]  # é è¨­ç‚ºç¬¬ 1 é 

        # **å­˜å…¥è³‡æ–™åº«**
        knowledge.content = "\n\n".join([text for _, text in extracted_pages])
        knowledge.save()

        # **å­˜å…¥å‘é‡è³‡æ–™åº«**
        for page_number, page_content in extracted_pages:
            add_to_enterprise_vectorstore(file.name, page_content, page_number)

        return Response(KnowledgeSerializer(knowledge).data, status=status.HTTP_201_CREATED)

class KnowledgeDetailView(generics.RetrieveUpdateDestroyAPIView):
    """
    - `PUT` æ›´æ–°æª”æ¡ˆ
    - `DELETE` åˆªé™¤æª”æ¡ˆ
    """
    queryset = Knowledge.objects.all()
    serializer_class = KnowledgeSerializer
    parser_classes = (MultiPartParser, FormParser)

    def put(self, request, *args, **kwargs):
        """æ›´æ–°çŸ¥è­˜æ–‡ä»¶ï¼Œåˆªé™¤èˆŠæ–‡ä»¶ä¸¦æ›´æ–°å‘é‡è³‡æ–™åº«"""

        knowledge = self.get_object()

        if "file" not in request.FILES:
            return Response({"error": "è«‹ä¸Šå‚³æ–°æ–‡ä»¶"}, status=status.HTTP_400_BAD_REQUEST)

        old_file_path = os.path.join(settings.MEDIA_ROOT, knowledge.file.name) if knowledge.file else None
        old_title = os.path.basename(knowledge.file.name) if knowledge.file else None  

        new_file = request.FILES["file"]

        # **å…ˆå­˜å…¥è‡¨æ™‚æª”æ¡ˆ (é¿å…æª”æ¡ˆéºå¤±)**
        temp_file_path = os.path.join(settings.MEDIA_ROOT, "temp_" + new_file.name)
        with default_storage.open(temp_file_path, 'wb+') as destination:
            for chunk in new_file.chunks():
                destination.write(chunk)

        # **æ›´æ–°è³‡æ–™åº«å…§å®¹**
        response = self.update(request, *args, **kwargs)

        # **ç¢ºä¿ Django å·²æ›´æ–° file å­—æ®µ**
        knowledge.refresh_from_db()

        if old_file_path and os.path.exists(old_file_path):
            os.remove(old_file_path)
            print(f"ğŸ—‘ å·²åˆªé™¤èˆŠæ–‡ä»¶: {old_file_path}")

        new_file_path = knowledge.file.path  # Django è‡ªå‹•å„²å­˜çš„æ–°è·¯å¾‘
        if new_file.name.endswith(".pdf"):
            extracted_pages = extract_text_from_pdf_with_pages(new_file_path)
        else:
            extracted_pages = [(1, extract_text_from_file(new_file_path))]

        knowledge.content = "\n\n".join([text for _, text in extracted_pages])
        knowledge.save()

        # **å¾å‘é‡è³‡æ–™åº«åˆªé™¤èˆŠæ–‡ä»¶å‘é‡**
        delete_from_enterprise_vectorstore(old_title)

        # **å­˜å…¥æ–°çš„å‘é‡è³‡æ–™åº«**
        for page_number, page_content in extracted_pages:
            add_to_enterprise_vectorstore(knowledge.file.name, page_content, page_number)

        return response
    
    def delete(self, request, *args, **kwargs):
        """åˆªé™¤ç‰¹å®šçŸ¥è­˜åº«æ–‡ä»¶ï¼Œä¸¦åŒæ­¥åˆªé™¤æœ¬åœ°æª”æ¡ˆ & å‘é‡è³‡æ–™"""
        knowledge = self.get_object()
        file_path = os.path.join(settings.MEDIA_ROOT, knowledge.file.name) if knowledge.file else None
        title = os.path.basename(knowledge.file.name) if knowledge.file else None

        if file_path and os.path.exists(file_path):
            os.remove(file_path)
            print(f"ğŸ—‘ å·²åˆªé™¤æœ¬åœ°æª”æ¡ˆ: {file_path}")

        # **å¾å‘é‡è³‡æ–™åº«åˆªé™¤**
        delete_from_enterprise_vectorstore(title)

        response = super().delete(request, *args, **kwargs)

        return response
    
    
class EnterpriseQueryView(generics.CreateAPIView):
    """ä¼æ¥­çŸ¥è­˜åº«æŸ¥è©¢ï¼Œæ”¯æ´é›²ç«¯/æœ¬åœ° LLMï¼Œä¸¦å¯é¸æ“‡æ˜¯å¦ä½¿ç”¨å‘é‡æª¢ç´¢"""

    serializer_class = EnterpriseQuerySerializer  # è®“ Swagger æ­£ç¢ºé¡¯ç¤º API åƒæ•¸
    def create(self, request, *args, **kwargs):
        query = request.data.get("query")
        model_type = request.data.get("model_type", "cloud")  # é è¨­ä½¿ç”¨é›²ç«¯ LLM
        use_retrieval = request.data.get("use_retrieval", True)  # é è¨­é–‹å•Ÿæª¢ç´¢

        if not query:
            return Response({"error": "Query is required"}, status=status.HTTP_400_BAD_REQUEST)

        print(f"ğŸ” [æŸ¥è©¢è«‹æ±‚] æ”¶åˆ°æŸ¥è©¢: {query}, ä½¿ç”¨æ¨¡å‹: {model_type}, å•Ÿç”¨æª¢ç´¢: {use_retrieval}")

        context = ""
        retrieved_docs = []

        if use_retrieval:
            # âœ… å–å¾—ä¼æ¥­çŸ¥è­˜åº«çš„ç›¸é—œæ–‡ä»¶
            retriever = enterprise_vectorstore.as_retriever(search_kwargs={"k": 5})
            documents = retriever.get_relevant_documents(query)

            if not documents:
                return Response({
                    "query": query,
                    "answer": "âš ï¸ æœªæ‰¾åˆ°ç›¸é—œå…§å®¹ï¼Œè«‹é‡æ–°è¼¸å…¥å•é¡Œæˆ–æä¾›æ›´å¤šç´°ç¯€ã€‚",
                    "retrieved_docs": []
                }, status=status.HTTP_200_OK)

            print(f"âœ… [æª¢ç´¢çµæœ] æ‰¾åˆ° {len(documents)} ä»½æ–‡ä»¶")

            # âœ… æå– `page_number` å’Œ `title`
            for doc in documents:
                page_number = doc.metadata.get("page_number", "æœªçŸ¥é ç¢¼")
                title = doc.metadata.get("title", "æœªçŸ¥æ–‡ä»¶")
                retrieved_docs.append({
                    "title": title,
                    "page_number": page_number,
                    "content": doc.page_content
                })

            # âœ… çµ„åˆ `context` ä½œç‚º LLM åƒè€ƒè³‡æ–™
            context = "\n\n".join([doc["content"] for doc in retrieved_docs])

        # âœ… å»ºç«‹ LLM æŸ¥è©¢ Prompt
        if use_retrieval:
            prompt = PromptTemplate(
                template="æ ¹æ“šä»¥ä¸‹èƒŒæ™¯è³‡è¨Šå›ç­”å•é¡Œï¼š\n\n{context}\n\nå•é¡Œï¼š{query}\nå›ç­”ï¼š",
                input_variables=["context", "query"]
            )
            formatted_prompt = prompt.format(context=context, query=query)
        else:
            prompt = PromptTemplate(
                template="è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼š\n\nå•é¡Œï¼š{query}\nå›ç­”ï¼š",
                input_variables=["query"]
            )
            formatted_prompt = prompt.format(query=query)

        print(f"ğŸ“œ [Prompt] é€å…¥ LLM:\n{formatted_prompt[:500]}...")

        # âœ… æ ¹æ“š `model_type` å‘¼å«ä¸åŒ LLM
        if model_type == "cloud":
            try:
                answer = AzureLlamaAPI.ask(formatted_prompt)
                print(f"ğŸ¤– [é›²ç«¯ LLM å›æ‡‰] {answer[:300]}...")
            except Exception as e:
                print(f"âŒ [é›²ç«¯ LLM éŒ¯èª¤] {str(e)}")
                answer = "âš ï¸ é›²ç«¯ LLM ä¼ºæœå™¨éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

        elif model_type == "local":
            try:
                llm = Ollama(model="openchat:latest")
                answer = llm.invoke(formatted_prompt)
                print(f"ğŸ¤– [æœ¬åœ° LLM å›æ‡‰] {answer[:300]}...")
            except Exception as e:
                print(f"âŒ [æœ¬åœ° LLM éŒ¯èª¤] {str(e)}")
                answer = "âš ï¸ æœ¬åœ° LLM ç„¡æ³•é‹è¡Œï¼Œè«‹æª¢æŸ¥è¨­ç½®ã€‚"

        else:
            return Response({"error": "Invalid model_type"}, status=status.HTTP_400_BAD_REQUEST)

        return Response({
            "query": query,
            "answer": answer,
            "retrieved_docs": retrieved_docs if use_retrieval else []
        }, status=status.HTTP_200_OK)
