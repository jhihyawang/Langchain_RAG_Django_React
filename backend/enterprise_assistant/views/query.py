# views/query.py

from common.modules.ai.llm_factory import LlmFactory
from common.modules.processor.vector_store import VectorStoreHandler
from drf_spectacular.utils import extend_schema
from enterprise_assistant.serializers import (
    EnterpriseQueryResponseSerializer, EnterpriseQuerySerializer)
from langchain_core.prompts import PromptTemplate
from rest_framework import status
from rest_framework.generics import CreateAPIView
from rest_framework.response import Response
from rest_framework.views import APIView


@extend_schema(
    request=EnterpriseQuerySerializer,
    responses=EnterpriseQueryResponseSerializer,
    summary="æŸ¥è©¢ä¼æ¥­çŸ¥è­˜åº« (æ”¯æ´ RAG)",
    description="ä½¿ç”¨è€…å¯è¼¸å…¥å•é¡Œï¼Œé¸æ“‡æ˜¯å¦å•Ÿç”¨æª¢ç´¢èˆ‡ä½¿ç”¨æœ¬åœ°æˆ–é›²ç«¯æ¨¡å‹ï¼Œç³»çµ±å°‡å›è¦†æ ¹æ“šçŸ¥è­˜åº«æ–‡ä»¶çš„å›ç­”ã€‚"
)

class EnterpriseQueryView(CreateAPIView):
    serializer_class = EnterpriseQuerySerializer

    def create(self, request, *args, **kwargs):
        serializer = self.get_serializer(data=request.data)
        serializer.is_valid(raise_exception=True)

        query = serializer.validated_data["query"]
        model_type = serializer.validated_data["model_type"]
        model_name = serializer.validated_data["model_name"]
        use_retrieval = serializer.validated_data["use_retrieval"]

        retrieved_docs = []
        context = ""
        vectorstore = VectorStoreHandler("chroma_user_db")

        if use_retrieval:
            retriever = vectorstore.vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 20})
            documents = retriever.invoke(query)

            if documents:
                print(f"âœ… [æª¢ç´¢çµæœ] æ‰¾åˆ° {len(documents)} ä»½æ–‡ä»¶")
                for doc in documents:
                    page_number = doc.metadata.get("page_number", "æœªçŸ¥é ç¢¼")
                    title = doc.metadata.get("title", "æœªçŸ¥æ–‡ä»¶")
                    retrieved_docs.append({
                        "title": title,
                        "page_number": page_number,
                        "content": doc.page_content
                    })
                    print(f"page_number {page_number} / title {title}: {doc.page_content[:100]}...")
                context = "\n\n".join([doc["content"] for doc in retrieved_docs])
            else:
                answer = "æˆ‘ä¸çŸ¥é“ï¼Œæˆ‘æ²’æœ‰è¢«æä¾›ç›¸é—œçš„çŸ¥è­˜æ–‡ä»¶ï¼Œä½ å¯ä»¥è©¦è©¦é—œé–‰æª¢ç´¢åŠŸèƒ½å†å•æˆ‘å•é¡Œ!!æ„Ÿæ©!!"
                response_data = {
                    "query": query,
                    "answer": answer,
                    "retrieved_docs": []
                }
                response_serializer = EnterpriseQueryResponseSerializer(response_data)
                return Response(response_serializer.data, status=status.HTTP_200_OK)

        prompt_template = PromptTemplate(
            template="æ ¹æ“šä»¥ä¸‹èƒŒæ™¯è³‡è¨Šå›ç­”å•é¡Œï¼š\n\n{context}\n\nå•é¡Œï¼š{query}\nå›ç­”ï¼š",
            input_variables=["context", "query"]
        ) if use_retrieval else PromptTemplate(
            template="è«‹æ ¹æ“šä½ çš„çŸ¥è­˜ç¯„åœå›ç­”å•é¡Œï¼š\n\nå•é¡Œï¼š{query}\nå›ç­”ï¼š",
            input_variables=["query"]
        )

        formatted_prompt = prompt_template.format(context=context, query=query) if use_retrieval else prompt_template.format(query=query)
        print(f"ğŸ“œ [Prompt] é€å…¥ LLM:\n{formatted_prompt[:500]}...")

        try:
            model = LlmFactory().create(model_type, model_name)
            answer = model.generate(formatted_prompt)
            print(f"[LLM å›æ‡‰] {answer[:300]}...")
        except Exception as e:
            print(f"[LLM éŒ¯èª¤] {str(e)}")
            answer = "âš ï¸ LLM ä¼ºæœå™¨éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

        response_data = {
            "query": query,
            "answer": answer,
            "retrieved_docs": retrieved_docs
        }
        response_serializer = EnterpriseQueryResponseSerializer(response_data)
        return Response(response_serializer.data, status=status.HTTP_200_OK)
