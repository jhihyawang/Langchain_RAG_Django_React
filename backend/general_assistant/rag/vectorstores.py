import os
import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForObjectDetection
from rest_framework import status
from rest_framework.response import Response
from rest_framework.views import APIView
from django.shortcuts import get_object_or_404
from ..models import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import ollama
import fitz  # PyMuPDF
import pdfplumber

# å‘é‡åº«åˆå§‹åŒ–
embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
CHROMA_user_DB_PATH = "chroma_user_db"
user_vectorstore = Chroma(persist_directory=CHROMA_user_DB_PATH, embedding_function=embedder)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)

device = "cuda" if torch.cuda.is_available() else "cpu"
processor = AutoProcessor.from_pretrained("microsoft/table-transformer-detection", revision="no_timm")
model = AutoModelForObjectDetection.from_pretrained("microsoft/table-transformer-detection", revision="no_timm").to(device)

# æ‘˜è¦

def summarize_text_or_table(element, element_type):
    prompt = f"ä½ æ˜¯ä¸€ä½æ–‡æœ¬è™•ç†çš„åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹å…§å®¹é€²è¡Œæ‘˜è¦ {element_type}ï¼š\n{element}"
    response = ollama.chat(
        model='gemma3:4b',
        messages=[{'role': 'user', 'content': prompt}]
    )
    return response.message.content

def summarize_image(image_path):
    prompt = "ä½ æ˜¯ä¸€ä½é‡å°å½±åƒå’Œåœ–ç‰‡é€²è¡Œæ‘˜è¦çš„åŠ©æ‰‹ï¼Œè«‹è©³ç´°æè¿°é€™å¼µåœ–ç‰‡çš„å…§å®¹ï¼Œè‹¥æ˜¯åœ–è¡¨è«‹èªªæ˜å…¶è¶¨å‹¢èˆ‡é—œéµæ•¸æ“š"
    try:
        response = ollama.chat(
            model='gemma3:4b',
            messages=[{'role': 'user', 'content': prompt, 'images': [image_path]}]
        )
        return response['message']['content']
    except Exception as e:
        return f"âŒ åœ–åƒåˆ†æéŒ¯èª¤: {str(e)}"

# å„²å­˜å‘é‡åº«

def add_to_general_vectorstore(title, content, page_number=1, document_id=None):
    if not content.strip():
        return False
    chunks = text_splitter.split_text(content)
    metadata = [{"title": title, "chunk_index": i, "page_number": page_number, "document_id": document_id} for i in range(len(chunks))]
    user_vectorstore.add_texts(chunks, metadatas=metadata)
    return True

# è¡¨æ ¼åµæ¸¬

def detect_and_crop_tables_from_image(image, page_index, output_dir="./table_images"):
    os.makedirs(output_dir, exist_ok=True)
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)

    target_sizes = torch.tensor([image.size[::-1]]).to(device)
    results = processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]

    saved_paths = []
    for i, box in enumerate(results["boxes"]):
        cropped = image.crop(box.tolist())
        save_path = os.path.join(output_dir, f"page{page_index+1}_table{i}.png")
        cropped.save(save_path)
        saved_paths.append((save_path, page_index+1))
    return saved_paths

# åœ–ç‰‡æ“·å–

def extract_images_from_pdf(pdf_path, output_dir="./pdf_images"):
    os.makedirs(output_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    saved = []
    for page_index in range(len(doc)):
        page = doc.load_page(page_index)
        images = page.get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            path = os.path.join(output_dir, f"page{page_index+1}_img{img_index+1}.{image_ext}")
            with open(path, "wb") as f:
                f.write(image_bytes)
            saved.append((path, page_index+1))
    return saved

# ä¸»æµç¨‹ï¼šè§£æ PDF ä¸­çš„æ‰€æœ‰å…ƒç´ 

def extract_element_from_pdf(file_path, knowledge_id=None):
    text_summaries, table_summaries, image_summaries = [], [], []

    # æ–‡å­—è™•ç†
    with pdfplumber.open(file_path) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            text = page.extract_text()
            if text:
                summary = summarize_text_or_table(text, "text")
                full_text = f"[æ‘˜è¦]{summary}[åŸæ–‡]{text}"
                add_to_general_vectorstore("Text Element", full_text, page_number=i, document_id=knowledge_id)
                text_summaries.append(summary)

    # è¡¨æ ¼è™•ç†ï¼ˆPDF è½‰åœ–åƒå¾Œæª¢æ¸¬ï¼‰
    from pdf2image import convert_from_path
    images = convert_from_path(file_path)
    for page_index, img in enumerate(images):
        tables = detect_and_crop_tables_from_image(img, page_index)
        for table_path, pg in tables:
            summary = summarize_image(table_path)
            add_to_general_vectorstore("Table Image", summary, page_number=pg, document_id=knowledge_id)
            table_summaries.append(summary)

    # åœ–ç‰‡æ“·å– + åˆ†æ
    images_from_pdf = extract_images_from_pdf(file_path)
    for img_path, page_number in images_from_pdf:
        summary = summarize_image(img_path)
        add_to_general_vectorstore("Embedded Image", summary, page_number=page_number, document_id=knowledge_id)
        image_summaries.append(summary)

    return {
        "text_summaries": text_summaries,
        "table_summaries": table_summaries,
        "image_summaries": image_summaries
    }


    
def delete_from_general_vectorstore(document_id):
    try:
        user_vectorstore._collection.delete(where={"document_id": document_id})
        print(f"âœ… å·²åˆªé™¤ document_id={document_id} çš„å‘é‡è³‡æ–™")
        return True
    except Exception as e:
        print(f"âŒ åˆªé™¤å‘é‡è³‡æ–™å¤±æ•—: {e}")
        return False

def list_from_general_vectorstore(document_id):
    try:
        results = user_vectorstore._collection.get(
            where={"document_id": document_id},
            include=["documents", "metadatas"]
        )
        documents = results.get("documents", [])
        metadatas = results.get("metadatas", [])
        ids = results.get("ids", [])
        if not documents:
            print(f"âš ï¸ æœªæ‰¾åˆ° document_id={document_id} çš„å‘é‡è³‡æ–™")
            return []
        chunks = []
        for i in range(len(documents)):
            metadata = metadatas[i] if i < len(metadatas) else {}
            chunks.append({
                "id": ids[i],
                "chunk_index": metadata.get("chunk_index", i),
                "page_number": metadata.get("page_number", None),
                "content": documents[i]
            })
        print(f"âœ… æ‰¾åˆ° {len(chunks)} ç­† document_id={document_id} çš„ chunks")
        return chunks
    except Exception as e:
        print(f"âŒ æŸ¥è©¢ document_id={document_id} çš„å‘é‡è³‡æ–™å¤±æ•—: {e}")
        return []

class ChunkListCreateView(APIView):
    def get(self, request, pk):
        document = get_object_or_404(Document, pk=pk)
        chunks = list_from_general_vectorstore(document.id)
        return Response({
            "document_id": document.id,
            "title": os.path.basename(document.file.name) if document.file else "",
            "chunks": chunks
        }, status=status.HTTP_200_OK)

class ChunkDetailView(APIView):
    """
    - `PUT` æ›´æ–°å–®ä¸€ chunk
    - `DELETE` åˆªé™¤å–®ä¸€ chunk
    """

    def put(self, request, chunk_id):
        content = request.data.get("content")
        if not content:
            return Response({"error": "è«‹æä¾›æ–°çš„ content"}, status=status.HTTP_400_BAD_REQUEST)

        try:
            # æŸ¥å‡ºåŸ metadata
            existing = user_vectorstore._collection.get(
                ids=[chunk_id],
                include=["metadatas"]
            )

            if not existing["metadatas"]:
                return Response({"error": "æ‰¾ä¸åˆ° chunk"}, status=status.HTTP_404_NOT_FOUND)

            metadata = existing["metadatas"][0]

            # åˆªé™¤èˆŠçš„å‘é‡
            user_vectorstore._collection.delete(ids=[chunk_id])

            # åŠ å…¥æ–°çš„å‘é‡ï¼ˆæ–°å…§å®¹ + èˆŠ metadataï¼‰
            user_vectorstore.add_texts(
                [content],
                metadatas=[metadata]
            )

            # å˜—è©¦æ›´æ–° document çš„ updated_at æ™‚é–“
            document_id = metadata.get("document_id")
            if document_id:
                from ..models import Document  # é¿å…å¾ªç’° import å•é¡Œ
                from django.utils.timezone import now

                try:
                    document = Document.objects.get(id=document_id)
                    # æ›´æ–° vectorstore å¾Œï¼Œé‡æ–°å–å¾—ç¬¬ä¸€å€‹ chunk ç•¶ä½œ content
                    chunks = list_from_general_vectorstore(document.id)
                    first_chunk = chunks[0]["content"] if chunks else ""
                    document.content = first_chunk
                    document.updated_at = now()
                    document.chunk = len(chunks)
                    document.save(update_fields=["content", "chunk", "updated_at"])
                    print(f"ğŸ“Œ å·²åŒæ­¥æ›´æ–° document ID={document_id} çš„ä¿®æ”¹æ™‚é–“")
                except Document.DoesNotExist:
                    print(f"âš ï¸ ç„¡æ³•æ‰¾åˆ° document ID={document_id}ï¼Œç„¡æ³•åŒæ­¥æ™‚é–“")

            return Response({"message": "âœ… å·²æ›´æ–° chunk"}, status=status.HTTP_200_OK)

        except Exception as e:
            return Response({"error": f"æ›´æ–°å¤±æ•—ï¼š{str(e)}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def delete(self, request, chunk_id):
        try:
            # å…ˆå–å¾— metadataï¼ˆå¯é¸ï¼Œç´”ç²¹ç‚ºäº† logï¼‰
            metadata = user_vectorstore._collection.get(
                ids=[chunk_id],
                include=["metadatas"]
            ).get("metadatas", [{}])[0]

            document_id = metadata.get("document_id", "æœªçŸ¥")
            user_vectorstore._collection.delete(ids=[chunk_id])
            print(f"ğŸ—‘ å·²åˆªé™¤ chunkï¼ˆID: {chunk_id}, document_id: {document_id}ï¼‰")

            return Response({"message": "âœ… å·²åˆªé™¤ chunk"}, status=status.HTTP_204_NO_CONTENT)

        except Exception as e:
            return Response({"error": f"åˆªé™¤å¤±æ•—ï¼š{str(e)}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
