import re
import pdfplumber
from paddleocr import PaddleOCR
from pdf2image import convert_from_path
import os
import numpy as np
from PIL import Image
import io
import torch
from transformers import DetrImageProcessor, TableTransformerForObjectDetection
from unstructured.partition.pdf import partition_pdf
import ollama
from difflib import SequenceMatcher

ocr_engine = PaddleOCR(use_angle_cls=True, lang='ch')

MEDIA_ROOT = "media"  # â¬…ï¸ è¨­å®š media è·¯å¾‘ç‚ºå…¨åŸŸ


def count_cid_like(text):
    cid_unicode_count = len(re.findall(r'[\ue000-\uf8ff]', text))
    cid_marker_count = len(re.findall(r'\(cid:\d+\)', text))
    return cid_unicode_count + cid_marker_count

def load_pdf_images_and_ocr(file_path, dpi=400):
    images = convert_from_path(file_path, dpi=dpi)
    ocr_cache = {}

    for i, image in enumerate(images):
        print(f"ğŸ–¼ï¸ OCR é è™•ç† ç¬¬ {i+1} é ")
        ocr_result = ocr_engine.ocr(np.array(image), cls=True)

        angles = [box[2].get("angle", 0) for line in ocr_result for box in line if len(box) >= 3 and isinstance(box[2], dict)]
        vertical_count = sum(1 for a in angles if a in [90, 270])
        ratio = vertical_count / len(angles) if angles else 0

        if len(angles) >= 5 and ratio > 0.6:
            dominant_angle = max(set(angles), key=angles.count)
            print(f"ğŸ”„ ç¬¬ {i+1} é éœ€è¦æ—‹è½‰ {dominant_angle} åº¦")
            image = image.rotate(-dominant_angle, expand=True)
            ocr_result = ocr_engine.ocr(np.array(image), cls=True)
        elif len(angles) < 5 and ratio > 0.3:
            dominant_angle = max(set(angles), key=angles.count)
            print(f"ğŸŒ€ å°‘é‡æ–‡å­—ä½†è§’åº¦ç•°å¸¸ï¼Œæ—‹è½‰ç¬¬ {i+1} é  {dominant_angle} åº¦")
            image = image.rotate(-dominant_angle, expand=True)
            ocr_result = ocr_engine.ocr(np.array(image), cls=True)

        images[i] = image
        ocr_cache[i + 1] = {
            "image": image,
            "ocr_result": ocr_result
        }

    return images, ocr_cache

def extract_text_from_pdf_with_fallback(images, ocr_cache, file_path, cid_threshold=5):
    results = []
    with pdfplumber.open(file_path) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            print(f"ğŸ“„ æ­£åœ¨è§£æç¬¬ {i} é ...", end=" ")
            text = page.extract_text() or ""
            cid_count = count_cid_like(text)

            if cid_count >= cid_threshold:
                print(f"CID é” {cid_count}ï¼Œä½¿ç”¨ OCR å¿«å–")
                ocr_result = ocr_cache.get(i, {}).get("ocr_result")
                ocr_text = "\n".join([line[1][0] for line in ocr_result[0]]) if ocr_result else ""

                results.append({
                    "page": i,
                    "source": "ocr",
                    "cid_count": cid_count,
                    "content": ocr_text.strip()
                })
            else:
                print(f"ä½¿ç”¨ pdfplumber:{text.strip()}")
                results.append({
                    "page": i,
                    "source": "origin",
                    "cid_count": cid_count,
                    "content": text.strip()
                })
    return results

# è¡¨æ ¼æ¨¡å‹åˆå§‹åŒ–
processor = DetrImageProcessor.from_pretrained("microsoft/table-transformer-detection")
table_model = TableTransformerForObjectDetection.from_pretrained("microsoft/table-transformer-detection")

def extract_title_above(image: Image.Image, box, ocr_engine, crop_height=200):
    top = max(box[1] - crop_height, 0)
    cropped = image.crop((box[0], top, box[2], box[1]))
    ocr_result = ocr_engine.ocr(np.array(cropped), cls=True)
    if ocr_result and isinstance(ocr_result[0], list):
        return "\n".join([line[1][0] for line in ocr_result[0]]).strip()
    return ""

def summarize_image(image_path, prompt):
    try:
        response = ollama.chat(
            model='gemma3:4b',
            messages=[{'role': 'user', 'content': prompt, 'images': [image_path]}]
        )
        return response['message']['content']
    except Exception as e:
        return f"âŒ åœ–åƒåˆ†æéŒ¯èª¤: {str(e)}"

def extract_table_and_summary(images, ocr_cache, save_dir="tables_valid"):
    output_dir = os.path.join(MEDIA_ROOT, save_dir)
    os.makedirs(output_dir, exist_ok=True)
    table_results = []

    for page_idx in range(1, len(images) + 1):
        print(f"ğŸ“„ åµæ¸¬ç¬¬ {page_idx} é è¡¨æ ¼...")

        image = ocr_cache[page_idx]["image"]
        inputs = processor(images=image, return_tensors="pt")
        with torch.no_grad():
            outputs = table_model(**inputs)

        target_sizes = torch.tensor([image.size[::-1]])
        results = processor.post_process_object_detection(outputs, threshold=0.7, target_sizes=target_sizes)[0]

        for i, box in enumerate(results["boxes"]):
            box = [int(v) for v in box.tolist()]
            cropped = image.crop((box[0], box[1], box[2], box[3]))
            file_name = f"page_{page_idx}_table_{i+1}.png"
            table_img_path = os.path.join(output_dir, file_name)
            cropped.save(table_img_path)

            title = extract_title_above(image, box, ocr_engine)
            if not title:
                title = f"ç¬¬ {page_idx} é è¡¨æ ¼ {i+1}"
            prompt = f"é€™æ˜¯ä¸€ä»½ç¾¤ç›Šè­‰åˆ¸112å¹´å ±ï¼Œé€™å¼µè¡¨æ ¼ç‚ºï¼š{title}ï¼Œè«‹è©³ç´°æè¿°é€™å¼µè¡¨æ ¼çš„å…§å®¹ï¼Œè‹¥æ˜¯æœ‰æ¯å€‹æ¬„ä½æœ‰é—œè¯æ€§ï¼Œè«‹ä¸€ä¸€åˆ—å‡ºæ¯ä¸€åˆ—ä¸­çš„æ‰€æœ‰é …ç›®ä»¥åŠå…¶å…§å®¹"
            summary = "æ¨¡æ“¬LLMæ‘˜è¦"
            table_results.append({
                "page": page_idx,
                "source": f"{save_dir}/{file_name}",
                "content": summary
            })

    return table_results

def extract_img_and_summary(file_path, ocr_cache, save_dir="images"):
    output_dir = os.path.join(MEDIA_ROOT, save_dir)
    os.makedirs(output_dir, exist_ok=True)
    raw_pdf_elements = partition_pdf(
        filename=file_path,
        extract_images_in_pdf=True,
        infer_table_structure=False,
        extract_image_block_output_dir=output_dir,
        hi_res_images=True,
    )
    results = []
    for e in raw_pdf_elements:
        if e.category == "Image" and hasattr(e.metadata, "image_path"):
            page_num = e.metadata.page_number or 0
            img_path = e.metadata.image_path
            try:
                img = Image.open(img_path)
                if ocr_cache.get(page_num):
                    ocr_result = ocr_engine.ocr(np.array(img), cls=True)
                else:
                    ocr_result = ocr_engine.ocr(np.array(img), cls=True)

                if ocr_result and len(ocr_result[0]) >= 1:
                    print(f"âœ… æœ‰æ–‡å­—ï¼Œä¿ç•™åœ–ç‰‡: {img_path}")
                    file_name = os.path.basename(img_path)
                    prompt = "è«‹è©³ç´°æè¿°é€™å¼µåœ–ç‰‡çš„å…§å®¹ï¼Œè‹¥æ˜¯åœ–è¡¨è«‹èªªæ˜å…¶è¶¨å‹¢èˆ‡é—œéµæ•¸æ“š"
                    summary = "æ¨¡æ“¬LLMæ‘˜è¦"
                    results.append({
                        "page": page_num,
                        "source": f"{save_dir}/{file_name}",
                        "content": summary
                    })
                else:
                    print(f"âš ï¸ ç„¡æ–‡å­—å…§å®¹ï¼Œåˆªé™¤åœ–ç‰‡: {img_path}")
                    os.remove(img_path)
            except Exception as err:
                print(f"âŒ åœ–ç‰‡è™•ç†å¤±æ•—: {img_path}ï¼ŒéŒ¯èª¤: {str(err)}")
    return results

def processData(file_path, document_id=None):
    print("ğŸ“¦ é å…ˆè¼‰å…¥åœ–åƒèˆ‡ OCR çµæœ...")
    images, ocr_cache = load_pdf_images_and_ocr(file_path)

    print("ğŸ“„ Extract text from PDF...")
    text_content = extract_text_from_pdf_with_fallback(images, ocr_cache, file_path, cid_threshold=5)

    print("ğŸ“Š Detecting tables...")
    table_content = extract_table_and_summary(images, ocr_cache)

    print("ğŸ–¼ï¸ Extracting images...")
    #img_content = extract_img_and_summary(file_path, ocr_cache)

    return {
        "text": text_content,
        "table": table_content,
        "image": []
    }
