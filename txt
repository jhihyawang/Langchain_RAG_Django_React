import io  # è™•ç†äºŒé€²ä½ä¸²æµï¼ˆå¦‚åœ–ç‰‡ã€PDF å½±åƒï¼‰
import json  # è™•ç† JSON åºåˆ—åŒ–/ååºåˆ—åŒ–
import os  # æª”æ¡ˆèˆ‡ç›®éŒ„æ“ä½œ
import re  # æ­£å‰‡è¡¨é”å¼ï¼Œç”¨æ–¼æ–‡å­—åŒ¹é…
import time  # è¨ˆæ™‚
from datetime import datetime  # å–å¾—ä¸¦æ ¼å¼åŒ–ç•¶å‰æ™‚é–“

import fitz  # PyMuPDFï¼Œç”¨æ–¼ PDF è®€å–èˆ‡æ¸²æŸ“
import numpy as np  # ç§‘å­¸é‹ç®—åº«
import ollama  # æœ¬åœ° LLM å‘¼å«
import pandas as pd  # è³‡æ–™è™•ç†ï¼Œç”¨æ–¼è¡¨æ ¼çµæ§‹
import pdfplumber  # PDF æ–‡å­—ã€è¡¨æ ¼èƒå–
import torch  # æ·±åº¦å­¸ç¿’é‹ç®—æ¡†æ¶
from easyocr import Reader  # OCR å·¥å…·
from pdf2image import convert_from_path  # æ–°å¢ï¼špdf2image è½‰ PDF ç‚º PIL åœ–åƒ
from PIL import Image, ImageChops, ImageDraw  # åœ–åƒè™•ç†
from transformers import AutoImageProcessor  # çµæ§‹è­˜åˆ¥å‰è™•ç†
from transformers import AutoModelForObjectDetection  # ç‰©ä»¶åµæ¸¬æ¨¡å‹
from transformers import AutoProcessor  # åµæ¸¬å‰è™•ç†
from transformers import TableTransformerForObjectDetection  # è¡¨æ ¼çµæ§‹åµæ¸¬æ¨¡å‹

from common.modules.processor.vector_store import VectorStoreHandler  # å‘é‡è³‡æ–™åº«æ“ä½œ


def log(msg):
    """
    çµ±ä¸€æ—¥èªŒè¼¸å‡ºï¼Œå‰ç¶´ [YYYY-MM-DD hh:mm:ss]
    """
    now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    print(f"{now} {msg}")


class PdfProcessor:
    """
    PDF å…¨æµç¨‹è™•ç†é¡åˆ¥ï¼Œæ”¯æ´ï¼š
    - åˆ¤æ–·æ˜¯å¦ OCR
    - æ–‡å­—æŠ½å–
    - åœ–ç‰‡æŠ½å–
    - è¡¨æ ¼åµæ¸¬èˆ‡è§£æ
    - LLM åœ–åƒé‡å»º
    - å‘é‡åº«å¯«å…¥
    """

    def __init__(
        self,
        pdf_path,
        vectorstore="chroma_user_db",
        output_dir="media",
        model_name="gemma3:27b",
        knowledge_id=None,
        knowledge_title="æœªçŸ¥æ–‡ä»¶",
        cid_threshold=20
    ):
        # --- åŸºæœ¬åƒæ•¸åˆå§‹åŒ– ---
        self.pdf_path = pdf_path
        self.file_stem = os.path.splitext(os.path.basename(pdf_path))[0]
        self.model_name = model_name
        self.knowledge_id = knowledge_id
        self.knowledge_title = knowledge_title
        self.cid_threshold = cid_threshold       # åˆ¤æ–· OCR é–¾å€¼
        self.dpi = 300                           # PDF æ¸²æŸ“è§£æåº¦
        
        # --- GPU / CPU è£ç½®é¸æ“‡ ---
        # è‹¥æœ‰å¯ç”¨ CUDAï¼Œå°±ä½¿ç”¨ GPUï¼Œå¦å‰‡å›è½åˆ° CPU
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.detect_device = self.device

        # --- å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾ ---
        self.output_dir = os.path.join(output_dir, "extract_data", self.file_stem)
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "tables"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "ocr_fallback"), exist_ok=True)

        # --- OCR Readerï¼ˆå•Ÿç”¨ GPU åŠ é€Ÿï¼‰---
        self.reader = Reader(['ch_tra', 'en'], gpu=torch.cuda.is_available())

        # --- è¡¨æ ¼åµæ¸¬æ¨¡å‹ & å‰è™•ç† ---
        self.detector = AutoModelForObjectDetection.from_pretrained(
            "microsoft/table-transformer-detection", revision="no_timm"
        ).to(self.detect_device)
        self.processor = AutoProcessor.from_pretrained(
            "microsoft/table-transformer-detection", revision="no_timm"
        )

        # --- è¡¨æ ¼çµæ§‹è­˜åˆ¥æ¨¡å‹ & å‰è™•ç† ---
        self.proc = AutoImageProcessor.from_pretrained(
            "microsoft/table-transformer-structure-recognition-v1.1-all"
        )
        # èª¿æ•´æœ€çŸ­é‚Šï¼Œæé«˜å°è¡¨æ ¼åµæ¸¬
        self.proc.size['shortest_edge'] = 800
        self.model = TableTransformerForObjectDetection.from_pretrained(
            "microsoft/table-transformer-structure-recognition-v1.1-all"
        ).to(self.device)

        # --- å‘é‡è³‡æ–™åº« Handler ---
        self.vectorstore = VectorStoreHandler(db_path=vectorstore)


    def should_ocr(self, text: str) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦éœ€è¦ OCRï¼š
        è‹¥æå–æ–‡å­—ä¸­åŒ…å«éå¤š CIDï¼ˆäº‚ç¢¼ï¼‰å­—å…ƒæˆ– (cid:123) æ¨™è¨˜ï¼Œ
        è¶…é cid_thresholdï¼Œå°±ä½¿ç”¨ OCR åˆ†æ”¯
        """
        cid_unicode = len(re.findall(r'[\ue000-\uf8ff]', text))
        cid_marker  = len(re.findall(r'\(cid:\d+\)', text))
        if cid_unicode + cid_marker >= self.cid_threshold:
            print(f"CID é” {cid_unicode + cid_marker}ï¼Œä½¿ç”¨ OCR")
            return True
        else:
            print("æ–‡å­—æ­£å¸¸ï¼Œä½¿ç”¨ pdfplumber")
            return False


    def summarize_image(self, image_paths, prompt: str) -> str:
        """
        å‘¼å«æœ¬åœ° Ollama LLMï¼Œå°å½±åƒé€²è¡Œå…§å®¹é‡å»º
        image_paths: åœ–ç‰‡è·¯å¾‘åˆ—è¡¨æˆ–å–®ä¸€è·¯å¾‘
        prompt: LLM æç¤ºè©
        """
        if isinstance(image_paths, str):
            image_paths = [image_paths]
        log("[è§£æåœ–ç‰‡æˆ–æ•´é å½±åƒ]")
        system_prompt = (
            "ä½ æ˜¯ä¸€ä½é‡å»ºåœ–ç‰‡å…§å®¹çš„åŠ©æ‰‹ã€‚"
            "ç›´æ¥é‚„åŸå…§å®¹ï¼Œä¸è¦é–‹å ´ç™½æˆ–é¡å¤–èªªæ˜ï¼Œåªå›å‚³ç´”å…§å®¹ã€‚"
        )
        try:
            resp = ollama.chat(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": prompt, "images": image_paths}
                ]
            )
            return resp['message']['content']
        except Exception as e:
            return f"âŒ åœ–åƒåˆ†æéŒ¯èª¤: {e}"


    def detect_rotation_angle_easyocr(self, ocr_result, min_text_count=5, vertical_angle_range=(75, 105)) -> int:
        """
        ä½¿ç”¨ EasyOCR çµæœåˆ¤æ–·æ˜¯å¦éœ€æ—‹è½‰é é¢ 90 åº¦ï¼š
        - è‹¥å‚ç›´æ–‡å­— >50% æˆ– çŸ­æ–‡å­—>40% æˆ– é«˜å¯¬æ¯”>40%ï¼Œæ—‹è½‰
        """
        vertical = short = tall = 0
        total   = len(ocr_result)
        for box, text, _ in ocr_result:
            # ç®—å‡ºæ–‡å­—æ¡†çš„è§’åº¦
            dx, dy = box[1][0]-box[0][0], box[1][1]-box[0][1]
            angle = abs(np.degrees(np.arctan2(dy, dx)))
            if vertical_angle_range[0] <= angle <= vertical_angle_range[1]:
                vertical += 1
            if len(text.strip()) <= 1:
                short += 1
            w = np.linalg.norm(np.array(box[0]) - np.array(box[1]))
            h = np.linalg.norm(np.array(box[0]) - np.array(box[3]))
            if h > w * 2:
                tall += 1
        if total < min_text_count:
            return 0
        if vertical/total > 0.5 or short/total > 0.4 or tall/total > 0.4:
            return 90
        return 0


    def calculate_table_area(self, box) -> float:
        """
        è¨ˆç®—å–®ä¸€è¡¨æ ¼æ¡†é¢ç©
        box: [x1, y1, x2, y2]
        """
        x1, y1, x2, y2 = box
        return abs((x2-x1)*(y2-y1))


    def extract_tables(self, img: Image.Image, page_num: int, idx: int, box) -> dict:
        """
        æª¢æ¸¬ä¸¦è§£æå–®ä¸€è¡¨æ ¼ï¼š
        1. æ“´å±•ç¯„åœå¾Œæˆªåœ–
        2. Table-Transformer ç‰©ä»¶åµæ¸¬
        3. Table-Transformer çµæ§‹è­˜åˆ¥
        4. æ‹†æ¬„ OCR
        å›å‚³ dict(page, media_type, source, content)
        """
        try:
            print("é–‹å§‹æå–è¡¨æ ¼")
            w, h = img.size
            coords = box.tolist()  # [x1,y1,x2,y2]
            x1 = max(0, coords[0]-50)
            y1 = max(0, coords[1]-50)
            x2 = min(w, coords[2]+50)
            y2 = min(h, coords[3]+50)
            if x2-x1 < 2 or y2-y1 < 2:
                return {"page":page_num, "media_type":"table", "source":None, "content":"âŒ ç¯„åœç•°å¸¸"}
            crop = img.crop((x1, y1, x2, y2))
            fname = f"page{page_num}_table{idx+1}.png"
            path  = os.path.join(self.output_dir, "tables", fname)
            crop.save(path)

            # ç‰©ä»¶åµæ¸¬
            inputs = self.proc(images=crop, return_tensors="pt")
            inputs = {k:v.to(self.device) for k,v in inputs.items()}
            with torch.no_grad():
                out = self.model(**inputs)
            tgt = torch.tensor([crop.size[::-1]]).to(self.device)
            results = self.proc.post_process_object_detection(out, threshold=0.5, target_sizes=tgt)[0]

            # è½‰æˆ DataFrame
            data = []
            for i, (lbl, box) in enumerate(zip(results["labels"], results["boxes"])):
                coords = [round(float(v),2) for v in box.tolist()]
                data.append({
                    "label": self.model.config.id2label[lbl.item()],
                    "x1": coords[0], "y1": coords[1],
                    "x2": coords[2], "y2": coords[3]
                })
            df = pd.DataFrame(data)

            # å¦‚æœæ²’æ‰¾åˆ°æ¬„é‚Šç•Œï¼Œæ•´è¡¨ OCR
            col_boxes = df[df.label=='table column'][['x1','x2']].to_numpy()
            if len(col_boxes)==0:
                ocr = self.reader.readtext(np.array(crop))
                merged = "\r\n".join([r[1] for r in ocr])
                return {"page":page_num, "media_type":"table","source":path,"content":merged}

            # å»é‡æ’åºæ¬„é‚Šç•Œ
            xs = np.unique(np.round(col_boxes).astype(int))
            clean = [xs[0]]
            for v in xs[1:]:
                if v-clean[-1]>=3:
                    clean.append(v)
            xs = np.array(clean)

            # é€æ¬„ OCR
            texts = []
            for i in range(len(xs)-1):
                x_start, x_end = xs[i], xs[i+1]
                if x_end-x_start<3:
                    texts.append("")
                    continue
                col_img = crop.crop((x_start, 0, x_end, crop.height))
                txt = "".join(self.reader.readtext(np.array(col_img), detail=0)).strip()
                texts.append(txt)
            content = "\n\n".join(texts)
            print(f"è¡¨æ ¼å…§å®¹ï¼š\n{content}")
            return {"page":page_num,"media_type":"table","source":path,"content":content}

        except Exception as e:
            log(f"âŒ è¡¨æ ¼æå–ä¾‹å¤–ï¼šç¬¬ {page_num} é , è¡¨æ ¼ {idx+1}ï¼š{e}")
            return {"page":page_num,"media_type":"table","source":None,"content":f"âŒ æå–å¤±æ•—ï¼š{e}"}
        
    def mask_table_regions(self,img: Image.Image, boxes):
        """
        æŠŠ det['boxes'] æ‰€æœ‰å€å¡Šå¡—é»‘ï¼Œå›å‚³æ–°çš„ PIL.Image
        """
        masked = img.copy()
        draw = ImageDraw.Draw(masked)
        for b in boxes:
            x1, y1, x2, y2 = map(int, b.tolist())
            draw.rectangle([x1, y1, x2, y2], fill="black")
        return masked

    def extract_texts(self, page, page_num, img=None) -> dict:
        """
        æ“·å–æ–‡å­—ï¼š
        - è‹¥ should_ocr å›å‚³ Trueï¼Œä¸”æœ‰ imgï¼Œå‰‡ OCR fallback
        - å¦å‰‡ä½¿ç”¨ pdfplumber ç›´æ¥æŠ½æ–‡å­—
        """
        try:
            text = page.extract_text() or ""
            if self.should_ocr(text) and img is not None:
                # OCR åˆ†æ”¯
                path = os.path.join(self.output_dir, "ocr_fallback", f"page_{page_num}.png")
                img.save(path)
                ocr = self.reader.readtext(np.array(img))
                txts = [t for _,t,conf in ocr if conf>0.5]
                merged = "".join(txts)
                return {"page":page_num,"media_type":"text","source":"ocr","content":merged}
            else:
                # åŸæ–‡å­—
                return {"page":page_num,"media_type":"text","source":"ori","content":text}

        except Exception as e:
            log(f"âŒ æ–‡å­—æŠ½å–å¤±æ•—ï¼šç¬¬ {page_num} é ï¼š{e}")
            return {"page":page_num,"media_type":"text","source":None,"content":f"âŒ æŠ½å–å¤±æ•—ï¼š{e}"}


    def extract_imgs(self, doc, page_num) -> list:
        """
        æ“·å–é é¢å…§æ‰€æœ‰åœ–ç‰‡ï¼ŒOCR å¿«ç¯© + LLM æè¿°
        """
        res = []
        try:
            page = doc.load_page(page_num-1)
            imgs = page.get_images(full=True)
            for idx, info in enumerate(imgs):
                xref = info[0]
                base = doc.extract_image(xref)
                img = Image.open(io.BytesIO(base["image"]))
                ocr = self.reader.readtext(np.array(img))
                txts = [t for _,t,conf in ocr if conf>0.5]
                merged = "".join(txts)
                if len(merged)<15:
                    log(f"âš ï¸ ç¬¬{page_num}é ç¬¬{idx+1}å¼µåœ–ç‰‡ OCRä¸è¶³ï¼Œè·³é")
                    continue
                ext = base["ext"]
                path = os.path.join(self.output_dir, "images", f"page_{page_num}_img_{idx+1}.{ext}")
                img.save(path)
                content = merged
                if len(merged)<30:
                    # LLM è©³è¿°
                    prompt = (
                        "è«‹æè¿°é€™å¼µåœ–ç‰‡å…§å®¹ï¼šè‹¥æ˜¯è¡¨æ ¼è«‹ç”¨ Markdownï¼›"
                        "è‹¥æ˜¯åœ–è¡¨è«‹èªªæ˜é¡å‹ã€è»¸ã€è³‡æ–™èˆ‡è¶¨å‹¢ã€‚"
                    )
                    content = self.summarize_image(path, prompt)
                res.append({"page":page_num,"media_type":"image","source":path,"content":content})
        except Exception as e:
            log(f"âŒ åœ–ç‰‡æŠ½å–å¤±æ•—ï¼šç¬¬ {page_num} é ï¼š{e}")
            res.append({"page":page_num,"media_type":"image","source":None,"content":f"âŒ å¤±æ•—ï¼š{e}"})
        return res


    def optimized_process(self):
        """
        æœ€ä½³åŒ–å…¨æµç¨‹ï¼š
        1. æ¨™è¨˜éœ€è¦æ¸²æŸ“çš„é é¢ï¼ˆè¡¨æ ¼ã€OCR æˆ–å…§å«åœ–ç‰‡ï¼‰
        2. ä½¿ç”¨ pdf2image è½‰æ›é€™äº›é é¢ç‚º PIL.Image
        3. ä¾è¡¨æ ¼é  / éè¡¨æ ¼é åˆ†åˆ¥è™•ç†
        4. å‘é‡è³‡æ–™åº«å¯«å…¥ & result.json æ›´æ–°
        5. å›å‚³æ‰€æœ‰çµæœèˆ‡æ—‹è½‰é ç¢¼
        """
        t0 = time.time()

        # 1. é–‹å•Ÿ PDF ç‰©ä»¶
        doc = fitz.open(self.pdf_path)
        pdf = pdfplumber.open(self.pdf_path)

        # 2. æ¨™è¨˜å“ªäº›é éœ€è¦æ¸²æŸ“æˆå½±åƒ
        need_img = set()
        table_pages = []
        for i, p in enumerate(pdf.pages, start=1):
            text = p.extract_text() or ""
            # å¦‚æœæœ‰è¡¨æ ¼å°±æ¨™ç‚ºè¡¨æ ¼é 
            if p.extract_tables():
                table_pages.append(i)
                need_img.add(i)
            # å¦‚æœæ–‡å­—äº‚ç¢¼æˆ– PyMuPDF åµæ¸¬åˆ°åœ–ç‰‡ï¼Œä¹Ÿè¦æ¸²æŸ“
            if self.should_ocr(text) or doc[i-1].get_images(full=True):
                need_img.add(i)

        # 3. ä½¿ç”¨ pdf2image ä¸€æ¬¡æŠŠæ‰€æœ‰é é¢è½‰ç‚º PIL.Image
        #    åªä¿ç•™ need_img è£¡çš„é é¢ï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨
        if need_img:
            log(f"ğŸ–¼ï¸ ä½¿ç”¨ pdf2image è½‰æ›é é¢ï¼š{sorted(need_img)}")
            # convert_from_path æœƒå›å‚³æ‰€æœ‰é çš„ Image list
            all_images = convert_from_path(self.pdf_path, dpi=self.dpi)
            # åªå–éœ€è¦çš„é é¢
            page_images = {i: all_images[i-1] for i in need_img}
        else:
            page_images = {}

        # 4. åˆå§‹åŒ– result.json
        result_path = os.path.join(self.output_dir, "result.json")
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump({"results": [], "rotated_pages": []}, f, ensure_ascii=False, indent=2)

        all_results = []
        rotated_pages = []

        # 5. é€é è™•ç†
        for i, page in enumerate(pdf.pages, start=1):
            log(f"ğŸ” ç¬¬ {i} é é–‹å§‹è™•ç†â€¦")

            # 5.1 å–å‡ºå·²ç”± pdf2image è½‰å¥½çš„å½±åƒï¼ˆè‹¥æœ‰ï¼‰
            img = page_images.get(i)  # è‹¥ i ä¸åœ¨ need_imgï¼Œimg æœƒæ˜¯ None

            page_results = []

            # 5.2 å¦‚æœæ˜¯è¡¨æ ¼é ï¼Œå…ˆåšæ—‹è½‰åµæ¸¬èˆ‡è¡¨æ ¼æŠ½å–
            if i in table_pages and img is not None:
                # ä½¿ç”¨ EasyOCR æ–‡å­—è§’åº¦åµæ¸¬
                ocr_res = self.reader.readtext(np.array(img))
                angle = self.detect_rotation_angle_easyocr(ocr_res)
                if angle == 90:
                    img = img.rotate(-90, expand=True)
                    rotated_pages.append(i)
                    log(f"ç¬¬ {i} é æ—‹è½‰ 90 åº¦")

                # Table-Transformer åµæ¸¬è¡¨æ ¼æ¡†
                inputs = self.processor(images=img, return_tensors="pt").to(self.detect_device)
                with torch.no_grad():
                    outputs = self.detector(**inputs)
                tgt = torch.tensor([img.size[::-1]]).to(self.detect_device)
                det = self.processor.post_process_object_detection(
                    outputs, threshold=0.5, target_sizes=tgt
                )[0]

                # é€å€‹æ¡†æŠ½å–è¡¨æ ¼
                for idx, box in enumerate(det["boxes"]):
                    tbl = self.extract_tables(img, i, idx, box)
                    page_results.append(tbl)

                # è‹¥è¡¨æ ¼ä½”æ¯”è¼ƒä½ï¼Œå†æŠ½å–æ–‡å­—èˆ‡åœ–ç‰‡
                page_area = img.width * img.height
                total_table_area = sum(self.calculate_table_area(b) for b in det["boxes"])
                # è‹¥è¡¨æ ¼ä½”æ¯” <50%ï¼Œå–åŒ…åœç›’ä¸Šä¸‹å…©å€‹å‰©é¤˜å€å¡Šï¼Œå†æŠ½æ–‡å­—èˆ‡åœ–ç‰‡
                if total_table_area / page_area < 0.5:
                    # a) å…ˆæŠŠæ‰€æœ‰å·²åµæ¸¬åˆ°çš„å°è¡¨æ ¼é®é»‘
                    masked_img = self.mask_table_regions(img, det["boxes"])

                    # b) äºŒæ¬¡ã€Œçµæ§‹åµæ¸¬ã€ï¼šç”¨ TableTransformerForObjectDetection æ¨¡å‹
                    inputs2 = self.proc(images=masked_img, return_tensors="pt")
                    inputs2 = {k: v.to(self.device) for k, v in inputs2.items()}
                    with torch.no_grad():
                        out2 = self.model(**inputs2)
                    tgt2 = torch.tensor([masked_img.size[::-1]]).to(self.device)
                    sec = self.proc.post_process_object_detection(
                        out2, threshold=0.5, target_sizes=tgt2
                    )[0]

                    # c) æŠŠåµæ¸¬åˆ°çš„ cell line çµæ§‹è½‰ DataFrame
                    data2 = []
                    for lbl, box in zip(sec["labels"], sec["boxes"]):
                        coords = [round(float(v), 2) for v in box.tolist()]
                        data2.append({
                            "label": self.model.config.id2label[lbl.item()],
                            "x1": coords[0], "y1": coords[1],
                            "x2": coords[2], "y2": coords[3]
                        })
                    df2 = pd.DataFrame(data2)

                    # d) å¦‚æœæœ‰åµæ¸¬åˆ°ã€Œtable columnã€æ¬„ä½ï¼Œè¡¨ç¤ºé€™å¡Šä»æ˜¯è¡¨æ ¼ â†’ å† extract_tables
                    col_boxes = df2[df2.label == 'table column'][['x1', 'x2']].to_numpy()
                    if len(col_boxes) > 0:
                        base_idx = len(det["boxes"])
                        for j, box2 in enumerate(sec["boxes"]):
                            page_results.append(
                                self.extract_tables(masked_img, i, base_idx + j, box2)
                            )
                    else:
                        # e) å¦å‰‡ç•¶ä½œç´”æ–‡å­—å€åŸŸï¼Œç›´æ¥ OCR/åŸæ–‡æŠ½å–
                        page_results.append(self.extract_texts(page, i, masked_img))

                    # f) åœ–ç‰‡éƒ¨åˆ†ä»ç”¨åŸåœ–æŠ½å–
                    page_results.extend(self.extract_imgs(doc, i))

            else:
                # 5.3 éè¡¨æ ¼é ï¼šç›´æ¥æŠ½æ–‡å­— & åœ–ç‰‡
                log("éè¡¨æ ¼é ï¼Œæå–æ–‡å­—å’Œåœ–ç‰‡")
                page_results.append(self.extract_texts(page, i, img))
                page_results.extend(self.extract_imgs(doc, i))

            # 6. å¯«å…¥å‘é‡è³‡æ–™åº« & æ›´æ–° result.json
            for item in page_results:
                if not item["content"]:
                    continue
                self.vectorstore.add(
                    content=item["content"],
                    page=item["page"],
                    document_id=self.knowledge_id,
                    media_type=item["media_type"],
                    title=self.knowledge_title,
                    source=item["source"]
                )

            with open(result_path, "r+", encoding="utf-8") as f:
                data = json.load(f)
                data["results"].extend(page_results)
                data["rotated_pages"] = rotated_pages
                f.seek(0)
                f.truncate()
                json.dump(data, f, ensure_ascii=False, indent=2)

            all_results.extend(page_results)
            log(f"âœ… ç¬¬ {i} é è™•ç†å®Œç•¢ï¼Œå¯«å…¥ {len(page_results)} ç­†ï¼›ç´¯è¨ˆæ—‹è½‰é ï¼š{rotated_pages}")

        # 7. æ¸…ç†ã€é—œé–‰è³‡æº
        pdf.close()
        doc.close()
        log(f"å…¨éƒ¨å®Œæˆï¼Œç”¨æ™‚ {time.time()-t0:.1f}sï¼Œç¸½è¨˜éŒ„æ•¸ {len(all_results)}")

        return all_results, rotated_pages
