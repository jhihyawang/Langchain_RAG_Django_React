import io
import json
import os
import re
import time
from datetime import datetime

import fitz  # PyMuPDF
import numpy as np
import ollama
import pdfplumber
import torch
from easyocr import Reader
from PIL import Image
from transformers import AutoModelForObjectDetection, AutoProcessor

from common.modules.processor.vector_store import VectorStoreHandler


def log(msg):
    now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    print(f"{now} {msg}")

class PdfProcessor:
    def __init__(self, pdf_path, vectorstore="chroma_user_db", output_dir="media", model_name="gemma3:12b", knowledge_id=None, knowledge_title="未知文件",cid_threshold=20):
        self.pdf_path = pdf_path
        self.file_stem = os.path.splitext(os.path.basename(pdf_path))[0]
        self.output_dir = os.path.join(output_dir,"extract_data",self.file_stem)
        self.model_name = model_name
        self.knowledge_id = knowledge_id
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.reader = Reader(['ch_tra', 'en'], gpu=torch.cuda.is_available())
        self.detector = AutoModelForObjectDetection.from_pretrained("microsoft/table-transformer-detection", revision="no_timm").to(self.device)
        self.processor = AutoProcessor.from_pretrained("microsoft/table-transformer-detection", revision="no_timm")
        self.cid_threshold = cid_threshold
        self.dpi = 200
        self.vectorstore = VectorStoreHandler(db_path=vectorstore)
        self.knowledge_title = knowledge_title
     
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "tables"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "ocr_fallback"), exist_ok=True)

    # def should_ocr(self, text):
    #     cid_count = text.count("(cid:")
    #     return cid_count > 15 or (len(text) > 0 and cid_count / len(text) > 0.3) or not bool(re.search(r"[\u4e00-\u9fa5a-zA-Z]", text))

    def should_ocr(self, text):
        cid_unicode_count = len(re.findall(r'[\ue000-\uf8ff]', text))
        cid_marker_count = len(re.findall(r'\(cid:\d+\)', text))
        cid_count = cid_unicode_count + cid_marker_count
        if cid_count >= self.cid_threshold:
            print(f"CID 達 {cid_count}，使用 OCR 快取")
            return True
        else:
            print(f"文字符合標準，使用pdfplumber")
            return False

    def summarize_image(self, image_paths, prompt):
        if isinstance(image_paths, str):
            image_paths = [image_paths]
        log("[解析圖片或整頁影像]")
        system_prompt = (
            "你是一位針對圖片影像和表格影像進行提取內容的助手"
            "請還原每張圖片中的文字段落、表格內容"
            "如果有提供ocr的文字辨識結果也可以參考，文字辨識可能有錯誤，若字詞不合邏輯必須轉換成正常的字詞"
            "若是圖表完整回傳每筆資料和其趨勢"
            "若是表格內容用markdown表格格式回傳結果"
        )
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt, "images": image_paths}
                ]
            )
            return response['message']['content']
        except Exception as e:
            return f"❌ 圖像分析錯誤: {str(e)}"

    def detect_rotation_angle_easyocr(self, ocr_result, min_text_count=5, vertical_angle_range=(75, 105)):
        vertical_texts, short_texts, tall_boxes = 0, 0, 0
        total_texts = len(ocr_result)
        for (box, text, _) in ocr_result:
            x0, y0 = box[0]; x1, y1 = box[1]
            dx, dy = x1 - x0, y1 - y0
            angle = abs(np.arctan2(dy, dx) * 180 / np.pi)
            if vertical_angle_range[0] <= angle <= vertical_angle_range[1]: vertical_texts += 1
            if len(text.strip()) <= 1: short_texts += 1
            w = np.linalg.norm(np.array(box[0]) - np.array(box[1]))
            h = np.linalg.norm(np.array(box[0]) - np.array(box[3]))
            if h > w * 2: tall_boxes += 1
        if total_texts < min_text_count: return 0
        if vertical_texts / total_texts > 0.5 or short_texts / total_texts > 0.4 or tall_boxes / total_texts > 0.4:
            return 90
        return 0
    
    def calculate_table_area(self,box) -> float:
        x1, y1, x2, y2 = box
        total_table_area = abs((x2 - x1) * (y2 - y1))
        return total_table_area
        
    def extract_table(self, img, page_num, idx, box):
        try:
            x1, y1, x2, y2 = box.tolist()
            # 1. 計算切割範圍
            left = max(0, x1-50)
            top  = max(0, y1-40)
            right= min(img.width, x2+50)
            bottom=min(img.height,y2+50)
            cropped = img.crop((left, top, right, bottom))
            
            # 2. 保存截图
            path = os.path.join(self.output_dir, "tables", f"page{page_num}_table{idx+1}.png")
            cropped.save(path)

            # 3. OCR 并过滤低置信度文字
            ocr_results = self.reader.readtext(np.array(cropped))
            texts = [txt for _, txt, conf in ocr_results if conf > 0.5]
            merged = "\r\n".join(texts)
            
            # 4. 调用 LLM 生成摘要
            prompt = (
                f"以下是一張表格截圖，OCR 出來的文字為：\n{merged}\n"
                "請根據截圖和文字還原表格結構，用 markdown 表格格式回傳純內容。"
            )
            summary = self.summarize_image(path, prompt)

            result = {
                "page": page_num,
                "media_type": "table",
                "source": path,
                "content": summary
            }
            return result
        
        except Exception as e:
            # 记录出错页码与索引，返回一个错误标识的结果以保证流程不中断
            log(f"❌ 第 {page_num} 页第 {idx+1} 个表格提取异常：{e}")
            
            return {
                "page": page_num,
                "media_type": "table",
                "source": None,
                "content": f"❌ 表格提取失败：{e}"
            }
            
    def extract_texts(self, page, page_num, img=None):
        try:
            text = page.extract_text() or ""
            if self.should_ocr(text) and img is not None:
                # OCR 路径分支
                path = os.path.join(self.output_dir, "ocr_fallback", f"page_{page_num}.png")
                img.save(path)
                ocr_results = self.reader.readtext(np.array(img))
                # 过滤置信度
                texts = [t for _, t, conf in ocr_results if conf > 0.5]
                merged = "\r\n".join(texts) if texts else ""
                prompt = (
                    f"以下圖片是一頁PDF文件的原始內容和擷取的文字如下：\n{merged}\n"
                    "請以原始文字內容重構這段文字段落，還原其原文，若有表格請還原為markdown表格。"
                )
                summary = self.summarize_image(path, prompt)
                content = f"[ocr]{merged}\n\n[llm]{summary}"
                source  = "ocr+llm"
            else:
                # 原文路径分支
                content = text
                source  = "ori"

            return {
                "page": page_num,
                "media_type": "text",
                "source": source,
                "content": content
            }

        except Exception as e:
            log(f"❌ 第 {page_num} 頁文字抽取异常：{e}")
            return {
                "page": page_num,
                "media_type": "text",
                "source": None,
                "content": f"❌ 文字抽取失败：{e}"
            }

    def extract_imgs(self, doc, page_num):
        results = []
        try:
            fitz_page = doc.load_page(page_num - 1)
            image_list = fitz_page.get_images(full=True)
            for img_index, img_info in enumerate(image_list):
                xref = img_info[0]
                base = doc.extract_image(xref)
                image_bytes, ext = base["image"], base["ext"]
                img = Image.open(io.BytesIO(image_bytes))
                ocr_results = self.reader.readtext(np.array(img))
                # 过滤置信度
                texts = [t for _, t, conf in ocr_results if conf > 0.5]
                merged = "\r\n".join(texts)
                if len(merged) < 10:
                    log(f"⚠️ 第 {page_num} 頁第 {img_index+1} 張圖片 OCR 結果不足，跳過")
                    continue

                path = os.path.join(self.output_dir, "images", f"page_{page_num}_img_{img_index+1}.{ext}")
                img.save(path)

                prompt = (
                    "請描述這張圖片的內容。如果是文章請還原文章的所有文字內容，如果是表格要用和圖片中同樣的表格架構呈現文字內容以及相對應的欄位，並用markdown表格格式回傳結果"
                    "請直接描述內容，不需要任何開場白、說明或引導句。"
                    "僅提供純內容，不要加上:[好的]、[以下是分析]等字句"
                    "若圖片為圖表，請指出:\r\n"
                    "1.圖表類型(如折線圖、長條圖等)\r\n"
                    "2.X軸和Y軸代表的意義\r\n"
                    "3.每筆資料的數值和名稱\r\n"
                    "4.資料呈現的趨勢、最高或最低點以及其關鍵變化\r\n"
                    "若圖片非圖表，請改為描述其主要構成與重要資訊"
                    )
                summary = self.summarize_image(path, prompt)

                results.append({
                    "page": page_num,
                    "media_type": "image",
                    "source": path,
                    "content": summary
                })

        except Exception as e:
            log(f"❌ 第 {page_num} 頁圖片抽取異常：{e}")
            results.append({
                "page": page_num,
                "media_type": "image",
                "source": None,
                "content": f"❌ 圖片抽取失敗：{e}"
            })

        return results
    
    def optimized_process(self):
        t0 = time.time()
        doc = fitz.open(self.pdf_path)
        pdf = pdfplumber.open(self.pdf_path)

        # 标记需要渲染的页码
        need_img = set()
        table_pages = []
        for i, p in enumerate(pdf.pages, start=1):
            text = p.extract_text() or ""
            if p.extract_tables():
                table_pages.append(i)
                need_img.add(i)
            if self.should_ocr(text) or doc[i-1].get_images(full=True):
                need_img.add(i)

        result_path = os.path.join(self.output_dir, "result.json")
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump({"results": [], "rotated_pages": []}, f, ensure_ascii=False, indent=2)

        all_results = []
        rotated_pages = []

        for i, page in enumerate(pdf.pages, start=1):
            log(f"🔍 第 {i} 页开始处理…")

            # 按需渲染
            img = None
            if i in need_img:
                pix = doc.load_page(i-1).get_pixmap(dpi=self.dpi)
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            page_results = []

            # —— 1. 表格抽取（先旋转，再检测）
            if i in table_pages and img is not None:
                # 1.1 旋转检测
                ocr_res = self.reader.readtext(np.array(img))
                angle = self.detect_rotation_angle_easyocr(ocr_res)
                if angle == 90:
                    img = img.rotate(-90, expand=True)
                    rotated_pages.append(i)
                    log(f"第 {i} 页旋转 90 度")

                # 1.2 表格检测
                inputs = self.processor(images=img, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    outputs = self.detector(**inputs)
                target_sizes = torch.tensor([img.size[::-1]]).to(self.device)
                det = self.processor.post_process_object_detection(
                    outputs, threshold=0.5, target_sizes=target_sizes
                )[0]

                # 1.3 测算表格占比
                page_area = img.width * img.height
                total_table_area = sum(
                    abs((x2-x1)*(y2-y1)) for (x1,y1,x2,y2) in det["boxes"]
                )
                table_ratio = total_table_area / page_area if page_area>0 else 0.0
                log(f"第 {i} 页表格占比：{table_ratio:.2%}")

                # 1.4 抽取所有表格
                for idx, box in enumerate(det["boxes"]):
                    tbl = self.extract_table(img, i, idx, box)
                    page_results.append(tbl)

                if table_ratio < 0.48:
                    # 3. 文本抽取：直接传 img 而不是 {i: img}
                    txt = self.extract_texts(page, i, img)
                    page_results.append(txt)
                    # 4. 图片抽取
                    imgs = self.extract_imgs(doc, i)
                    page_results.extend(imgs)
                else:
                    log(f"第 {i} 页表格占比≥48%，跳过文字和图片抽取")
            else:
                # 非表格页，或者没有渲染图时
                txt = self.extract_texts(page, i, img)
                page_results.append(txt)
                imgs = self.extract_imgs(doc, i)
                page_results.extend(imgs)

            # 写入向量库
            for item in page_results:
                self.vectorstore.add(
                    content=item["content"],
                    page=item["page"],
                    document_id=self.knowledge_id,
                    media_type=item["media_type"],
                    title=self.knowledge_title,
                    source=item["source"]
                )

            # 更新 result.json
            with open(result_path, "r+", encoding="utf-8") as f:
                data = json.load(f)
                data["results"].extend(page_results)
                data["rotated_pages"] = rotated_pages
                f.seek(0); f.truncate()
                json.dump(data, f, ensure_ascii=False, indent=2)

            all_results.extend(page_results)
            log(f"✅ 第 {i} 页处理完毕，写入 {len(page_results)} 条记录；累计旋转页：{rotated_pages}")

            # 清理 OCR 缓存
            ocr_tmp = os.path.join(self.output_dir, "ocr_fallback", f"page_{i}.png")
            if os.path.exists(ocr_tmp):
                os.remove(ocr_tmp)

        pdf.close()
        doc.close()
        log(f"🏁 全部完成，用时 {time.time()-t0:.1f}s，总记录数 {len(all_results)}")
        return all_results, rotated_pages

if __name__ == "__main__":
    processor = PdfProcessor("test.pdf")
    result = processor.optimized_process()


